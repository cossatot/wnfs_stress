{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gdal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pyproj as pj\n",
    "import halfspace.load as hs\n",
    "import halfspace.sandbox as hbx\n",
    "import time\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dem_dataset = gdal.Open('../data/dem/wnfs_dem_utm45_500m.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x_rotation': 0.0, 'y_res_m': 500.0, 'y_rotation': 0.0, 'x_res_m': 500.0, 'n_cols': 2474, 'n_rows': 2197, 'upper_left_y': 3775544.3200867064, 'upper_left_x': -368334.0655201413}\n"
     ]
    }
   ],
   "source": [
    "# write GeoTransform parameters to dictionary\n",
    "\n",
    "dem_transform = {'upper_left_x': dem_dataset.GetGeoTransform()[0],\n",
    "                 'x_res_m' : dem_dataset.GetGeoTransform()[1],\n",
    "                 'x_rotation': dem_dataset.GetGeoTransform()[2],\n",
    "                 'upper_left_y': dem_dataset.GetGeoTransform()[3],\n",
    "                 'y_rotation': dem_dataset.GetGeoTransform()[4],\n",
    "                 'y_res_m': dem_dataset.GetGeoTransform()[5],\n",
    "                 'n_cols': dem_dataset.RasterXSize,\n",
    "                 'n_rows': dem_dataset.RasterYSize}\n",
    "\n",
    "dem_transform['y_res_m'] *= -1 # correct for the upper-left origin thing\n",
    "\n",
    "print(dem_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dem_transform['east_min'] = dem_transform['upper_left_x']\n",
    "\n",
    "dem_transform['east_max'] = (dem_transform['x_res_m'] * dem_transform['n_cols']\n",
    "                             + dem_transform['east_min'])\n",
    "\n",
    "dem_transform['north_max'] = dem_transform['upper_left_y']\n",
    "\n",
    "dem_transform['north_min'] = (-1 * dem_transform['y_res_m'] * dem_transform['n_rows']\n",
    "                              + dem_transform['north_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get latitude and longitude for the corners of the array, just 'cause.\n",
    "\n",
    "wgs84 = pj.Proj(init='epsg:4326')\n",
    "utm45n = pj.Proj(init='epsg:32645')\n",
    "\n",
    "dem_transform['lon_min'], dem_transform['lat_min'] = pj.transform(utm45n, \n",
    "                                                        wgs84,\n",
    "                                                        dem_transform['east_min'], \n",
    "                                                        dem_transform['north_min'])\n",
    "\n",
    "dem_transform['lon_max'], dem_transform['lat_max'] = pj.transform(utm45n, \n",
    "                                                        wgs84,\n",
    "                                                        dem_transform['east_max'], \n",
    "                                                        dem_transform['north_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/dem/wnfs_dem_utm45_meta.json', 'w') as f:\n",
    "    json.dump(dem_transform, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read DEM data as numpy array\n",
    "dem = dem_dataset.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "dem = dem.astype('float')\n",
    "\n",
    "dem.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some nodata values are likely present\n",
    "\n",
    "dem[dem < 0] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dem = np.flipud(dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stress_dir = '../stress_arrays/'\n",
    "b_stress_file = stress_dir + 'wnfs_bouss_stress.h5'\n",
    "c_stress_file = stress_dir + 'wnfs_cerr_stress.h5'\n",
    "stress_file = stress_dir + 'wnfs_topo_stress.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rho = 2700  # density in kg m^-3\n",
    "g = 9.81    # gravitational force in m s^-2\n",
    "Fv = - rho * g\n",
    "study_res = int(dem_transform['x_res_m']) # resolution for topography, filters, etc.\n",
    "z_res = 500\n",
    "b_conv_mode = 'valid'\n",
    "c_conv_mode = 'same'\n",
    "\n",
    "z_min = z_res\n",
    "z_max = z_min + 25000\n",
    "z_len = int( (z_max - z_min) / z_res + 1)\n",
    "z_vec = np.linspace(z_min, z_max, num=z_len)\n",
    "\n",
    "kernel_rad = 2e5\n",
    "kernel_len = int( kernel_rad * 2 / study_res +1 )\n",
    "kernel_shape = np.array( [kernel_len, kernel_len] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dem *= -1 # topo is negative in our convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "b_out_x, b_out_y = hbx.size_output(kernel_shape, dem.shape,\n",
    "                                   mode=b_conv_mode)\n",
    "\n",
    "b_out_size = np.array((b_out_x, b_out_y, z_len))\n",
    "b_stress_empty = np.zeros( (b_out_size))\n",
    "\n",
    "b_db = h5py.File(b_stress_file, mode='w')\n",
    "b_dict = {}\n",
    "comp_list = ['xx', 'yy', 'zz', 'xy', 'xz', 'yz']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on xx stresses\n",
      "92.22466731071472 secs for convolution\n",
      "working on yy stresses\n",
      "98.39397478103638 secs for convolution\n",
      "working on zz stresses\n",
      "86.62586998939514 secs for convolution\n",
      "working on xy stresses\n",
      "89.7741014957428 secs for convolution\n",
      "working on xz stresses\n",
      "86.83986139297485 secs for convolution\n",
      "working on yz stresses\n",
      "86.87778353691101 secs for convolution\n",
      "done with Boussinesq calcs in 13.305543609460194 m\n"
     ]
    }
   ],
   "source": [
    "t2 = time.time()\n",
    "for comp in comp_list:\n",
    "    print('working on {} stresses'.format(comp) )\n",
    "    b_dict[comp] = b_stress_empty.copy()\n",
    "    \n",
    "    t_conv_start = time.time()\n",
    "    for i, z in enumerate(z_vec):\n",
    "        b_dict[comp][:,:,i] = hs.do_b_convo(component=comp,  z=z, \n",
    "                                            load=dem, \n",
    "                                            Fv=Fv, load_mode='topo',\n",
    "                                            conv_mode=b_conv_mode, \n",
    "                                            kernel_radius=kernel_rad,\n",
    "                                            kernel_res=study_res)\n",
    "    t_conv_stop = time.time()\n",
    "    print(t_conv_stop - t_conv_start, 'secs for convolution')\n",
    "    b_dict[comp] *= 1e-6  # scale results to MPa\n",
    "    \n",
    "    b_db.create_dataset('b_{}_MPa'.format(comp), data = b_dict[comp],\n",
    "                     chunks = True, compression = 'gzip')\n",
    "\n",
    "    del b_dict[comp]\n",
    "\n",
    "print('done with Boussinesq calcs in', (time.time() - t2) / 60., 'm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Boussinesq stresses for xx, xy and yy are used in the loading function\n",
    "b_xx_top = b_db['b_xx_MPa'][:,:,0] * 1e6\n",
    "b_xy_top = b_db['b_xy_MPa'][:,:,0] * 1e6\n",
    "b_yy_top = b_db['b_yy_MPa'][:,:,0] * 1e6\n",
    "b_shape = b_xx_top.shape\n",
    "\n",
    "topo = hs._centered(dem, b_shape)\n",
    "\n",
    "topo_dy, topo_dx = np.gradient(topo, study_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make horizontal loading functions\n",
    "Fht_x = topo * Fv * topo_dx\n",
    "Fhb_x = b_xx_top * topo_dx + b_xy_top * topo_dy \n",
    "\n",
    "Fht_y = topo * Fv * topo_dy\n",
    "Fhb_y = b_yy_top * topo_dy + b_xy_top * topo_dx\n",
    "\n",
    "Fh_x = Fht_x + Fhb_x\n",
    "Fh_y = Fht_y + Fhb_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make Cerruti output arrays\n",
    "c_x = np.zeros([topo.shape[0], topo.shape[1], z_len])\n",
    "c_y = c_x.copy()\n",
    "c_db = h5py.File(c_stress_file)\n",
    "t_db = h5py.File(stress_file)\n",
    "\n",
    "del topo # save some ram\n",
    "\n",
    "cerr_x = {}\n",
    "cerr_y = {}\n",
    "total_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on xx stresses\n",
      "saving xx data\n",
      "adding all results together\n",
      "working on yy stresses\n",
      "saving yy data\n",
      "adding all results together\n",
      "working on zz stresses\n",
      "saving zz data\n",
      "adding all results together\n",
      "working on xy stresses\n",
      "saving xy data\n",
      "adding all results together\n",
      "working on xz stresses\n",
      "saving xz data\n",
      "adding all results together\n",
      "working on yz stresses\n",
      "saving yz data\n",
      "adding all results together\n",
      "done with topo corrections in 21.20552794933319 m\n"
     ]
    }
   ],
   "source": [
    "t3 = time.time()\n",
    "for comp in comp_list:\n",
    "    print('working on {} stresses'.format(comp))\n",
    "\n",
    "    cerr_x[comp] = c_x.copy()\n",
    "    cerr_y[comp] = c_y.copy()\n",
    "\n",
    "    for i, z in enumerate(z_vec):\n",
    "        cerr_x[comp][:,:,i] = hs.do_c_convo(component=comp, f_dir='x',z=z,\n",
    "                                            load=Fh_x, kernel_res=study_res,\n",
    "                                            kernel_radius=kernel_rad,\n",
    "                                            conv_mode=c_conv_mode) * 1e-6\n",
    "\n",
    "        cerr_y[comp][:,:,i] = hs.do_c_convo(component=comp, f_dir='y', z=z,\n",
    "                                            load=Fh_y, kernel_res=study_res,\n",
    "                                            kernel_radius=kernel_rad,\n",
    "                                            conv_mode=c_conv_mode) * 1e-6\n",
    "\n",
    "    print('saving {} data'.format(comp))\n",
    "    c_db.create_dataset('c_{}_x_MPa'.format(comp), \n",
    "                      data = cerr_x[comp], chunks=True, compression = 'gzip')\n",
    "\n",
    "    c_db.create_dataset('c_{}_y_MPa'.format(comp), \n",
    "                      data = cerr_y[comp], chunks=True, compression = 'gzip')\n",
    "\n",
    "    print('adding all results together')\n",
    "    total_dict[comp] = (b_db['b_{}_MPa'.format(comp)][:,:,:] +  cerr_x[comp] \n",
    "                        + cerr_y[comp] )\n",
    "\n",
    "    t_db.create_dataset('{}_MPa'.format(comp), data=total_dict[comp],\n",
    "                        chunks = True, compression = 'gzip')\n",
    "\n",
    "    del total_dict[comp]\n",
    "    del cerr_x[comp]\n",
    "    del cerr_y[comp]\n",
    "\n",
    "print('done with topo corrections in', (time.time() - t3) / 60., 'm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b_db.close()\n",
    "c_db.close()\n",
    "t_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
